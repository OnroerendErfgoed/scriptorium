{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOmLm5SzXZuxl83unJhmzpf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OnroerendErfgoed/scriptorium/blob/main/notebooks/11_all_getty_skos_matches.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All your match are belong to us\n",
        "\n",
        "Sometimes an API doesn't exactly provide you with a filter or search parameter to do what you want. In those cases you might need to make more effort to get the results that you want, but because all data is available through our services, there's nothing you can't do with a little script. Bear in mind that this can put some strain on a server. If you're doing a lot of calls to a service (Onroerend Erfgoed or somebody else) it might be a good idea to reach out to the server administrator and ask them if they're ok with what you're doing. They might even know a better way to get the results you're after.\n",
        "\n",
        "One good way of not overloading a server is to make sure you wait a bit between calls. This makes your script slower, but it ensures the server does not get overloaded and stays alive.\n",
        "\n",
        "```python\n",
        "import time\n",
        "\n",
        "# Sleep for half a second\n",
        "time.sleep(0.5)\n",
        "```\n",
        "\n",
        "For this script we'll work with the [Onroerend Erfgoed Thesaurus](https://thesaurus.onroerenderfgoed.be) to demonstrate downloading all data and doing the data-processing we're after client-side. We'll use the Conceptscheme of `Erfgoedtypes` to see how many of them are linked to the [Getty AAT thesaurus](http://www.getty.edu/research/tools/vocabularies/aat/). Basically, we want to know which `Erfgoedtypes` have a link (called a Match in [SKOS](https://www.w3.org/2004/02/skos/) termminology) with a concept in the AAT.\n",
        "\n",
        "The thesaurus API supports checking which concepts are linked to a certain AAT concept, but it does not support searching for all concepts that are linked to any AAT concept."
      ],
      "metadata": {
        "id": "LyLoihz67SBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "THESAURUS_HOST = 'https://thesaurus.onroerenderfgoed.be'\n",
        "ERFGOEDTYPES_URL = THESAURUS_HOST + '/conceptschemes/ERFGOEDTYPES/c'\n",
        "\n",
        "session = requests.Session()\n",
        "\n",
        "session.headers.update({'Accept': 'application/json'})\n",
        "\n",
        "res = session.get(\n",
        "    ERFGOEDTYPES_URL,\n",
        "    params={\n",
        "        'match': 'http://vocab.getty.edu/aat/300005241'\n",
        "    }\n",
        ")\n",
        "\n",
        "print(res.json())"
      ],
      "metadata": {
        "id": "bTXdNDVy8XLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous example works well. It does tell us which concept is linked to this AAT concept, but we would like to see all concepts in this thesaurus that have a link to the AAT. We could query the thesaurus with every single AAT URI, but there are a lot of concepts in the AAT, so this would take forever.\n",
        "\n",
        "Since our thesaurus of `Erfgoedtypes` is much smaller than the AAT, we'll GET every concept in it and check to see if there's a link with the AAT. We'll also save the information to a CSV file. This does require authentication your Google Drive. Also make sure the variable *OUTPUT_FILE* points to a valid folder that is writeable.\n",
        "\n",
        "To make testing the script easier we've added a parameter *MAX_OUTPUT* that stops the script once a certain number of matches has been reached. "
      ],
      "metadata": {
        "id": "t5_Vay76-tOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')\n",
        "\n",
        "THESAURUS_HOST = 'https://thesaurus.onroerenderfgoed.be'\n",
        "ERFGOEDTYPES_URL = THESAURUS_HOST + '/conceptschemes/ERFGOEDTYPES/c'\n",
        "\n",
        "# Set to a number to terminate the script when a certain number of results has been reached\n",
        "MAX_OUTPUT = None\n",
        "OUTPUT_FILE = '/drive/My Drive/Colab Notebooks/skos_matches.csv'\n",
        "\n",
        "session = requests.Session()\n",
        "\n",
        "session.headers.update({'Accept': 'application/json'})\n",
        "\n",
        "# Get all erfgoedtypes\n",
        "# This collection does not support pagination and always sends everything\n",
        "res = session.get(\n",
        "    ERFGOEDTYPES_URL,\n",
        "    params = {\n",
        "        'type': 'concept'\n",
        "    }\n",
        ")\n",
        "\n",
        "# Make sure everything went well\n",
        "res.raise_for_status()\n",
        "\n",
        "data = res.json()\n",
        "\n",
        "output = []\n",
        "\n",
        "for concept in data:\n",
        "  print(f\"Processing {concept['uri']}\")\n",
        "  conceptresponse = session.get(concept['uri'])\n",
        "  conceptresponse.raise_for_status()\n",
        "  conceptdetail = conceptresponse.json()\n",
        "  # Only look at concepts that have any matches at all\n",
        "  if conceptdetail['matches']:\n",
        "    for matchtype, matchvalues in conceptdetail['matches'].items():\n",
        "      # matchvalue might be a list\n",
        "      for matchvalue in matchvalues:\n",
        "        if matchvalue.startswith('http://vocab.getty.edu'):\n",
        "          print(f\"Match found between {concept['uri']} and {matchvalue}\")\n",
        "          output.append([concept['uri'], concept['label'], matchtype, matchvalue])\n",
        "  # We handled a concept, now sleep a bit\n",
        "  time.sleep(0.2)\n",
        "  # Once we have MAX_OUTPUT rows, stop\n",
        "  if MAX_OUTPUT and len(output) >= MAX_OUTPUT:\n",
        "    break\n",
        "\n",
        "with open(OUTPUT_FILE, 'w') as csvfile:\n",
        "  outputwriter = csv.writer(csvfile)\n",
        "  for row in output:\n",
        "    outputwriter.writerow(row)"
      ],
      "metadata": {
        "id": "uA7UKfUg--nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "This script shows us that even if the API doesn't provide the perfect filter for us to use, we can still access all the data and process it ourselves.\n",
        "\n",
        "This example could be written in a different way using the [Skosprovider](https://github.com/OnroerendErfgoed/skosprovider) family of libraries, including [one](https://github.com/OnroerendErfgoed/skosprovider_getty) that makes it much easier to talk to the Getty services."
      ],
      "metadata": {
        "id": "wWMzFoOfAwUl"
      }
    }
  ]
}